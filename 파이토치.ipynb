{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789390b8",
   "metadata": {},
   "source": [
    "# 4.1 인공 신경망의 한계와 딥러닝 출현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811af7a7",
   "metadata": {},
   "source": [
    "오늘날 인공 신경망에서 이용하는 구조는 프랭크 로젠블라트가 1957년에 고안한 퍼셉트론이라는 선형 분류기이다. 퍼셉트론은 오늘날 신경망(딥러닝)의 기원이 되는 알고리즘입니다.\n",
    "\n",
    "<img src=\"Perceptron.png\" align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6380158",
   "metadata": {},
   "source": [
    "## ADN게이트  \n",
    "\n",
    "<img src=\"ANDgate.png\" align=\"left\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e219beb",
   "metadata": {},
   "source": [
    "데이터 분류로 표현하게 되면,  \n",
    "    \n",
    "<img src=\"AND게이트.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d519c0",
   "metadata": {},
   "source": [
    "## -OR게이트  \n",
    "\n",
    "<img src=\"ORgate.png\" align=\"left\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68ddba",
   "metadata": {},
   "source": [
    "데이터 분류로 표현하게 되면,  \n",
    "    \n",
    "<img src=\"OR게이트.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d99194",
   "metadata": {},
   "source": [
    "## XOR 게이트\n",
    "\n",
    "<img src=\"XORgate.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee8d9b",
   "metadata": {},
   "source": [
    "데이터 분류로 표현하게 되면, 아래 그림과 같이 분류할수 있는데 XOR게이트는 앞에서의 데이터들과 달리 비선형적으로 분리 되기 때문에 분류가 어렵습니다. 즉, 퍼셉트론에서는 AND, OR연산에 대해 학습이 가능하지만, XOR에 대해서는 학습이 불가능합니다.\n",
    "    \n",
    "<img src=\"XOR게이트.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2af24c",
   "metadata": {},
   "source": [
    "이를 극복하는 방안으로 입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 두어 비선형적으로 분리되는 데이터에 대해서도 학습이 가능하도록 다층 퍼셉트론(multi-layer perceptron)을 고안했습니다.\n",
    "\n",
    "이때 입력층과 출력층 사이에 은닉층이 여러 개 있는 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 하며, 심층 신경망을 다른 이름으로 딥러닝이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ab0f2",
   "metadata": {},
   "source": [
    "# 4.2 딥러닝 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb46584",
   "metadata": {},
   "source": [
    "딥러닝이란 여러 층을 가진 인공 신경망을 사용하여 학습을 수행하는 것  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cdd05b",
   "metadata": {},
   "source": [
    "## 4.2.1 딥러닝 용어  \n",
    "\n",
    "- 딥러닝의 구조\n",
    "\n",
    "<img src=\"deeplearning structure.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442006c9",
   "metadata": {},
   "source": [
    "- 딥러닝 구성 요소  \n",
    "\n",
    "<img src=\"DeeplearningComponent.png\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca48891",
   "metadata": {},
   "source": [
    "### 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1410731",
   "metadata": {},
   "source": [
    "- 가중치는 입력 값이 연산 결과에 미치는 영향력을 조절하는 요소  \n",
    "- 입력 값의 연산 결과를 조정하는 역할을 하는 것  \n",
    "  \n",
    "<img src=\"가중치.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9548c",
   "metadata": {},
   "source": [
    "### 가중합 또는 전달함수\n",
    "\n",
    "- 각 노드에서 들어오는 신호에 가중치를 곱해서 다음 노드로 전달되는데 이 값들을 모두 더한 합계를 뜻함.\n",
    "    - 노드의 가중합이 계산된 후 활성화 함수로 보내기 때문에 전달 함수(transfer function)라고도 불림  \n",
    "    \n",
    "<img src=\"전달함수.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d998720",
   "metadata": {},
   "source": [
    "- 가중합 공식: $\\displaystyle\\sum_{i}^{}{w_ix_i+b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d2e38",
   "metadata": {},
   "source": [
    "### 활성화함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ce7dd",
   "metadata": {},
   "source": [
    "- 활성화함수란?\n",
    "    - 전달 함수에서 전달받은 값을 출력할 떄 일정 기준에 따라 출력값을 변화시키는 비선형 함수\n",
    "    - 활성화 함수 종류\n",
    "        - 시그모이드 함수\n",
    "        - 하이퍼볼릭 탄젠트\n",
    "        - 렐루\n",
    "        - 리키렐루\n",
    "        - 소프트맥스\n",
    "        \n",
    "#### 1. 시그모이드 함수\n",
    "- 시그모이드 함수 : 선형 함수의 결과를 0~1사이에서 비선형 형태로 변형해 줌.\n",
    "    - 로지스틱 회귀와 같은 분류문제를 확률적으로 표현하는데 사용\n",
    "    - 딥러닝 모델의 깊이가 깊어지면 즉, 은닉층이 많아지면 많아질수록 기울기가 사라지는 '기울기 소멸 문제(vanishing gradient problem)'가 발생하게 됨\n",
    "    \n",
    "<img src=\"SigmoidFunc.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b2f7f",
   "metadata": {},
   "source": [
    "# $f(x) = \\frac {1}{1+e^-1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a35c6",
   "metadata": {},
   "source": [
    "#### 2. 하이퍼볼릭 탄젠트 함수\n",
    "- 하이퍼볼릭 탄젠트 함수\n",
    "    - 선형 함수의 결과를 -1~1 사이에서 비선형 형태로 변형해 줌\n",
    "    - 시그모이드 함수에서 결과값을 평균이 0이 아닌 양수로 편향된 문제를 해결\n",
    "    - 기울기 소멸 문제는 여전히 발생\n",
    "    \n",
    "<img src=\"HyperbolicFunc.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e28b81",
   "metadata": {},
   "source": [
    "#### 3. 렐루 함수\n",
    "- 렐루 함수\n",
    "    - 최근 활발히 사용되고 있다.\n",
    "    - 입력(x)이 음수일 때는 0을 출력하고, 양수일 때는 x를 출력\n",
    "    - 경사 하강법(gradient descent)에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않는다.\n",
    "    - 일반적으로 은닉층에서 사용\n",
    "    - 하이퍼볼릭 탄젠트 함수 대비 학습 속도가 6배 빠름\n",
    "    - 음수 값을 입력받으면 항상 0을 출력하기 때문에 학습 능력이 감소하게 되는 단점이 있음.\n",
    "    \n",
    "<img src=\"LeluFunc.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb6684",
   "metadata": {},
   "source": [
    "#### 4. 리키 렐루 함수\n",
    "- 리키 렐루 함수\n",
    "    - 입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환\n",
    "    - 입력 값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제를 해결\n",
    "    \n",
    "<img src=\"LeakyReluFunc.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf044b",
   "metadata": {},
   "source": [
    "#### 5. 소프트맥스 함수\n",
    "- 소프트맥스 함수\n",
    "    - 입력 값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 하는 함수\n",
    "    - 보통 딥러닝에서 출력 노드의 활성화 함수로 많이 사용  \n",
    "    \n",
    "    - 수식 표현:  \n",
    "    $y_k = \\frac{exp(a_k)}{\\displaystyle\\sum_{i=1}^{n}{exp(a_i)}}$\n",
    "      \n",
    "    (n = 출력층의 뉴런 개수, $y_k$ = k번째 출력, $a_k$ = k번째 입력신호)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d52eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden) #------ 은닉층\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output) #------ 출력층\n",
    "        self.softmax = torch.nn.Softmax(dim=n_output)\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x) #------ 은닉층을 위한 렐루 활성화 함수\n",
    "        x = self.out(x)\n",
    "        x = self.softmax(x) #------ 출력층을 위한 소프트맥스 활성화 함수\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67d06ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden): Linear(in_features=1, out_features=2, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (out): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=3)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net(1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907147a",
   "metadata": {},
   "source": [
    "### 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef29eab",
   "metadata": {},
   "source": [
    "- 손실함수 : 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표\n",
    "    - 값이 클수록 많이 틀렸다는 의미이고, 이 값이 ‘0’에 가까우면 완벽하게 추정할 수 있다는 의미\n",
    "    - 대표적인 손실 함수\n",
    "        - 평균 제곱 오차(Mean Squared Error, MSE)\n",
    "        - 크로스 엔트로피 오차(Cross Entropy Error, CEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee370a40",
   "metadata": {},
   "source": [
    "#### 평균 제곱 오차(MSE)\n",
    "- 평균 제곱 오차(MSE) : 실제 값과 예측 값의 차이(error)를 제곱한 평균\n",
    "- 실제 값과 예측 값의 차이와 예측력은 반비례\n",
    "- 평균 제곱 오차를 구하는 수식:  \n",
    "  $MSE = \\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}{(y_i-\\hat{y_i})}^2$  \n",
    "  ($\\hat{y_i}$ : 신경망의 출력(신경망이 추정한 값),$y_i$: 정답 레이블, i: 데이터의 차원 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c293c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "y_pred = model(x)\n",
    "loss = loss_fn(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6885c9",
   "metadata": {},
   "source": [
    "#### 크로스 엔트로피 오차\n",
    "- 크로스 엔트로피 오차 : 분류 문제에서 원-핫 인코딩 했을때만 사용할 수 있는 오차 계산법\n",
    "- 경사 하강법 과정에서 학습이 지역 최소점에서 멈출 수 있음.\n",
    "    \n",
    "    \n",
    "$CrossEntropy = -\\displaystyle\\sum_{i=1}^{n}{y_i\\log_{}\\hat{y_i}}$  \n",
    "  ($\\hat{y_i}$ : 신경망의 출력(신경망이 추정한 값),$y_i$: 정답 레이블, i: 데이터의 차원 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    " #------ torch.randn은 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용하여 숫자를 생성\n",
    "input = torcSh.randn(5, 6, requires_grad=True)\n",
    " #------ torch.empty는 dtype torch.float32의 랜덤한 값으로 채워진 텐서를 반환\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb8f78",
   "metadata": {},
   "source": [
    "## 4.2.2. 딥러닝 학습\n",
    "- 딥러닝 학습 단계\n",
    "    - 순전파  \n",
    "    - 역전파  \n",
    "<img src=\"Deeplearning.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bff839",
   "metadata": {},
   "source": [
    "첫 번째 단계인 순전파(feddforward)는 네트워크에 푼련 데이터가 들어올 때 발생하며, 데이터를 기반으로 예측 값을 계산하기 위해 전체 신경망을 교차해 지나가게 된다. 즉, 모든 뉴런이 이전 층의 뉴런에서 수신한 정보에 변환(가중합 및 활성화 함수)을 적용하여 다음 층(은닉층)의 뉴런으로 전송하는 방식   \n",
    "  \n",
    "네트워크를 통해 입력 데이터를 전달하며, 데이터가 모든 층을 통과하고 모든 뉴런이 계산을 완료하면 그 예측 값은 최종 층(출력층)에 도달  \n",
    "  \n",
    "손실 함수로 네트워크의 예측 값과 실제 값 차이(손실, 오류)를 추정  \n",
    "  \n",
    "손실 함수 비용이 0에 가깝도록 하기 위해 모델이 훈련을 반복하면서 가중치를 조정합니다. 손실이 계산되면 그 정보는 역으로 전파(출력층 → 은닉층 → 입력층)된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f125d",
   "metadata": {},
   "source": [
    "## 4.2.3. 딥러닝의 문제점과 해결 방안"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ec540",
   "metadata": {},
   "source": [
    "- 딥러닝의 핵심: 활성화 함수가 적용된 여러 은닉층을 결합하여 비선형 영역을 표현하는 것\n",
    "- 활성화 함수와 적용된 은닉층 개수가 많을수록 데이터 분류가 잘 된다.  \n",
    "\n",
    "<img src=\"DataClassification.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bad96",
   "metadata": {},
   "source": [
    "#### 과적합 문제 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e0e14",
   "metadata": {},
   "source": [
    "- 과적합: 훈련 데이터를 과하게 학습해서 발생\n",
    "- 학습을 과하게 하게되면 예측 값과 실제 값의 차이가 감소하지만 검증 데이터에 대해서는 오차가 증가할 수 있음  \n",
    "- 이러한 관점에서 과적합은 훈련 데이터에 대해 과하게 학습하여 실제 데이터에 대한 오차가 증가하는 현상을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd18f9",
   "metadata": {},
   "source": [
    "<img src=\"Overfitting.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540dd90d",
   "metadata": {},
   "source": [
    "##### 드롭아웃 : 이러한 과적합되는 현상을 줄이기 위해 학습 과정 중 임의로 일부 노드들을 학습에서 제외시킴  \n",
    "<img src=\"Dropout.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98c55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2), #------드롭아웃 적용\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaafc82",
   "metadata": {},
   "source": [
    "#### 기울기 소멸 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f625055",
   "metadata": {},
   "source": [
    "기울기 소멸 문제는 은닉층이 많은 신경망에서 주로 발생하는데 출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상  \n",
    "해당 현상은 시그모이드나 하이퍼볼릭 탄젠트 대신 렐루 활성화 함수를 사용하게 되면 해결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc9185",
   "metadata": {},
   "source": [
    "<img src=\"기울기소멸문제.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28126b90",
   "metadata": {},
   "source": [
    "#### 성능이 나빠지는 문제 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3011f",
   "metadata": {},
   "source": [
    "- 경사 하강법은 손실 함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동 시키는 과정을 반복하는데, 이때 성능이 나빠지는 문제가 발생\n",
    "- 이러한 문제점을 개선하고자 확률적 경사 하각법과 미니 배치 경사 하강법을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a38f7",
   "metadata": {},
   "source": [
    "<img src=\"경사하강법유형.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec609016",
   "metadata": {},
   "source": [
    "##### 배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c6934",
   "metadata": {},
   "source": [
    "- 배치 경사 하강법: 전체 데이터셋에 대한 오류를 구한 후 기울기를 한 번만 계산하여 모델의 파라미터를 업데이트 하는 방법. 즉, 전체  훈련 데이터셋에 대해 가중치를 편미분 하는 방법\n",
    "                                        \n",
    "                                  손실 함수의 값을 최소화하기 위해 기울기 이용\n",
    "                                  ┌┐\n",
    "배치 경사 하강법 수식: $W=W-\\alpha\\nabla J(W,b)$  \n",
    "$(\\alpha: 학습률, J:손실 함수)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c447f8",
   "metadata": {},
   "source": [
    "배치 경사 하각법은 학습이 오래 걸리는 단점이 있는데 이 단점을 개선한 방법을 확률적 경사 하강법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24682d6",
   "metadata": {},
   "source": [
    "##### 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bd0c1",
   "metadata": {},
   "source": [
    "- 확률적 경사 하강법(Stochastic Gradient Descent, SGD):   \n",
    "  임의로 선택한 데이터에 대해 기울기를 계산하는 방법으로 적은 데이터를 사용하므로 빠른 계산이 가능\n",
    "    - 파라미터 변경 폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮다.\n",
    "    - 속도가 빠르다는 장점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880a0a8",
   "metadata": {},
   "source": [
    "<img src=\"확률적경사하강법.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a778fe",
   "metadata": {},
   "source": [
    "##### 미니 배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d2b70",
   "metadata": {},
   "source": [
    "- 미니 배치 경사 하강법(mini-batch gradient descent)\n",
    "- 전체 데이터셋을 미니 배치(mini-batch) 여러 개로 나누고, 미니 배치 한 개마다 기울기를 구한 후 그것의 평균 기울기를 이용하여 모델을 업데이트해서 학습하는 방법\n",
    "- 속도가 빠르다\n",
    "- 확률적 경사 하강법 보다 안정적이다.\n",
    "- 이러한 장점들로 인해 실제로 가장 많이 사용하고있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b59a88",
   "metadata": {},
   "source": [
    "<img src=\"미니배치경사하강법.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90953966",
   "metadata": {},
   "source": [
    "#### 옵티마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f09c37",
   "metadata": {},
   "source": [
    "<img src=\"Optimizer.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e512d3",
   "metadata": {},
   "source": [
    "##### 속도를 조정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d454bf",
   "metadata": {},
   "source": [
    "###### 아다그라드\n",
    "- 아다그라드: 변수(가중치)의 업데이트 횟수에 따라 학습률을 조정하는 방법\n",
    "- 많이 변화하지 않는 변수들의 학습률은 크게 하고, 많이 변화하는 변수들의 학습률은 작게 한다.\n",
    "\n",
    "- 파라미터마다 다른 학습률을 주기 위해 G함수를 추가\n",
    "- 기울기가 크면 G 값이 커지기 때문에 에서 학습률(η)은 작아집니다. 즉, 파라미터가 많이 학습되었으면 작은 학습률로 업데이트되고, 파라미터 학습이 덜 되었으면 개선의 여지가 많기 때문에 높은 학습률로 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225c287",
   "metadata": {},
   "source": [
    "$w(i+1)=w(i)-\\frac{\\eta}{\\sqrt{G(i)+\\varepsilon}}\\nabla E(w(i))$  \n",
    "$G(i)=G(i-1)+(\\nabla E(w(i)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23823ff9",
   "metadata": {},
   "source": [
    "기울기가 크면 G 값이 커지기 때문에 $\\frac{\\eta}{\\sqrt{G(i)+\\varepsilon}}$ 에서 학습률(η)은 작아집니다. 즉, 파라미터가 많이 학습되었으면 작은 학습률로 업데이트되고, 파라미터 학습이 덜 되었으면 개선의 여지가 많기 때문에 높은 학습률로 업데이트된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcc054",
   "metadata": {},
   "source": [
    "###### 아다델타(Adadelta, Adaptive delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb72ac2",
   "metadata": {},
   "source": [
    "아다델타는 아다그라드에서 G 값이 커짐에 따라 학습이 멈추는 문제를 해결하기 위해 등장한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c0c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adadelta\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(rho=0.95),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853faa1",
   "metadata": {},
   "source": [
    "##### 알엠에스프롭(RMSProp)\n",
    "- 알엠에스프롭은 아다그라드의 G(i) 값이 무한히 커지는 것을 방지하고자 제안된 방법\n",
    "- 아다그라드에서 학습이 안 되는 문제를 해결하기 위해 G 함수에서 γ(감마)만 추가\n",
    "- γ:  학습률 크기를 비율로 조정하여 학습이 안되는 것을 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a45a5",
   "metadata": {},
   "source": [
    "##### 운동량을 조절하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500b830",
   "metadata": {},
   "source": [
    "##### 모멘텀(Momentum)\n",
    "- 경사 하강법과 마찬가지로 매번 기울기를 구하지만, 가중치를 수정하기 전에 이전 수정 방향(+, -)을 참조하여 같은 방향으로 일정한 비율만 수정하는 방법  \n",
    "- 수정이 양(+)의 방향과 음(-)의 방향으로 순차적으로 일어나는 지그재그 현상이 줄어들고, 이전 이동 값을 고려하여 일정 비율만큼 다음 값을 결정하므로 관성 효과를 얻을 수 있는 장점\n",
    "- 모멘텀은 SDG(확률적 경사 하강법)와 함께 사용됨\n",
    "\n",
    "확률적 경사 하강법(SDG) 수식: $w(i+1)=w(i)-\\eta \\nabla E(w(i))$   \n",
    "  \n",
    "모멘텀 수식은 다음과 같다.  \n",
    "$w(i+1)=w(i)-v(i)$  \n",
    "$v(i)=γv(i-1)+(\\eta \\nabla E(w(i))$\n",
    "\n",
    "모멘텀은 확률적 경사 하강법에서 기울기를 속도로 대체하여 사용하는 방식으로, 이전 속도의 일정 부분을 반영하게된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab7171",
   "metadata": {},
   "source": [
    "$\\eta \\nabla E(w(i))$ 수식을 사용하여 가중치를 계산하게 된다.\n",
    "기울기 크기와 반대 방향 만큼 가중치를 업데이트 하게된다. (기울기가 크면 아래쪽 방향으로 업데이트)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c3a526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD()의 파라미터로 모멘텀을 지정합니다.\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d515e3",
   "metadata": {},
   "source": [
    "##### 네스테로프 모멘텀(Nesterov Accelerated Gradient, NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce566e",
   "metadata": {},
   "source": [
    "- 모멘텀 값과 기울기 값이 더해져 실제 값을 만드는 기존 모멘텀과 달리 모멘텀 값이 적용된 지점에서 기울기 값을 계산\n",
    "- 모멘텀 방법은 멈추어야 할 시점에서도 관성에 의해 훨씬 멀리 갈 수 있는 단점이 있지만, 네스테로프 방법은 모멘텀으로 절반 정도 이동한 후 어떤 방식으로 이동해야 하는지 다시 계산하여 결정\n",
    "- 빠른 이동 속도는 그대로 가져가면서 멈추어야 할 적절한 시점에서 제동을 거는 데 훨씬 용이\n",
    "\n",
    "$w(i+1) = w(i) - v(i)$  \n",
    "$v(i)=γv(i-1)+\\eta \\nabla E(w(i)-γv(i-1))$  \n",
    "\n",
    "모멘텀과 비슷하지만 속도를 구하느 과정에서 조금 차이가 있고, 모멘텀과 다르게 이전에 학습했던 속도와 현재 기울기에서 이전 속도를 뺀 변화량을 반영하여 가중치를 구하게된다.\n",
    "\n",
    "\n",
    "<img src=\"NAG, Momentum.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c991d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD()의 파라미터로 nesterov=True를 지정하면 됩니다.\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3b21a",
   "metadata": {},
   "source": [
    "##### 운동량을 조절하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda01ea",
   "metadata": {},
   "source": [
    "##### 아담(Adam, Adaptive Moment Estimation)\n",
    "- 모멘텀과 알엠에스프롭의 장점을 결합한 경사 하강법\n",
    "- 알엠에스프롭 특징인 기울기의 제곱을 지수 평균한 값과 모멘텀 특징인 v(i)를 수식에 활용\n",
    "\n",
    "아담 수식:\n",
    "$w(i+1)=w(i)-\\frac{\\eta}{\\sqrt{G(i)+\\varepsilon}}v(i)$  \n",
    "$G(i) = γ_2G(i-1)+(1-γ_2)(\\nabla E(w(i)))^2$  \n",
    "$v(i) = γ_1v(i-1)+\\eta\\nabla E(w(i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64c6a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bac78c",
   "metadata": {},
   "source": [
    "## 4.2.4. 딥러닝을 사용할 때 이점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a0634",
   "metadata": {},
   "source": [
    "#### 특성 추출  \n",
    "컴퓨터가 입력받은 데이터를 분석하여 일정한 패턴이나 규칙을 찾아내려면 컴퓨터가 인지할 수 있는 데이터로 변환해 주어야 하는데 이때 데이터별 특징을 가지고 있는지 찾아내고, 그것을 토대로 데이터를 벡터로 변환하는 작업을 특성 추출(feature extraction)이라 한다.\n",
    "\n",
    "딥러닝을 사용하기 이전에는 머신 러닝 알고리즘인 SVM, 나이브 베이즈, 로지스틱 회귀의 특성 추출이 복잡하여 데이터에 대한 전문지식이 필요했지만, 딥러닝에서는 특징 추출 과정을 알고리즘에 통합시켰다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34fd26",
   "metadata": {},
   "source": [
    "#### 빅데이터의 효율적 활용\n",
    "딥러닝에서는 특성 추출을 알고리즘에 통합시켰다고 했는데 이것이 가능한 이유는 빅데이터 때문이다. 딥러닝 학습을 이용한 특징 추출은 데이터 사례가 많을수록 성능이 향상되기 때문입니다.\n",
    "\n",
    "다른 말로 표현하면 확보된 데이터가 적다면 딥러닝의 성능 향상을 기대하기 힘들기 때문에 머신 러닝을 고려해 보아야 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
